# AutoPatch+

**AutoPatch+** is a hallucination-aware verification and self-correction framework for Large Language Models (LLMs). It integrates retrieval-augmented generation (RAG), hallucination detection, and correction pipelines using interpretable models like Neural Additive Models (NAMs) to ensure response trustworthiness across QA and summarization tasks.

---

## ğŸ” Key Features

- âœ… **Hallucination Detection**: Uses both training-based (NAMs, LIME) and sampling-based (SelfCheckGPT) methods.
- ğŸ” **Self-Correction**: Automatically rewrites hallucinated outputs using retrieval context.
- ğŸ“Š **Interpretable Verification**: Uses interpretable models (e.g., NAMs) to justify answer correctness.
- ğŸ”— **Modular RAG Pipeline**: Supports various retrievers, vector stores, and LLM backends.
- ğŸŒ **Web UI**: Streamlit-based demo app to visualize detection and correction in real time.

---

## ğŸ—ï¸ Architecture

User Input --> RAG Pipeline --> LLM Output
|
Hallucination Detector
|
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Detected False Detected True
â†’ Accept â†’ â†’ Rewrite â†’

markdown
Copy
Edit

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- [transformers](https://github.com/huggingface/transformers)
- [streamlit](https://streamlit.io/)
- [scikit-learn](https://scikit-learn.org/)
- [openai](https://pypi.org/project/openai/)
- [langchain](https://github.com/langchain-ai/langchain)

Install dependencies:

```bash
pip install -r requirements.txt
Run the App
bash
Copy
Edit
streamlit run appfinal.py
Or for a backend evaluation without UI:

bash
Copy
Edit
python evaluate_hallucination.py
ğŸ§  Hallucination Detection Modes
SelfCheck Sampling: Compare multiple sampled LLM outputs for consistency.

NAM-Based Detector: Trains interpretable models on labeled hallucination data.

Cluster-Aware Verifier: Uses answer clustering to assign best detector dynamically.

ğŸ“ Repository Structure
bash
Copy
Edit
.
â”œâ”€â”€ appfinal.py                 # Streamlit frontend
â”œâ”€â”€ detector/
â”‚   â”œâ”€â”€ selfcheck.py            # Sampling-based hallucination detector
â”‚   â””â”€â”€ nam_verifier.py         # Interpretable NAM-based verifier
â”œâ”€â”€ rag_pipeline/
â”‚   â”œâ”€â”€ retriever.py            # Vector/keyword hybrid search
â”‚   â””â”€â”€ context_builder.py      # Document retrieval interface
â”œâ”€â”€ rewrite/
â”‚   â””â”€â”€ rewriter.py             # LLM-based rewriting for detected hallucinations
â”œâ”€â”€ data/                       # Datasets & QA examples
â”œâ”€â”€ eval/                       # Evaluation scripts & metrics
â””â”€â”€ README.md
ğŸ’¡ Example Use Case
Input a factual question.

AutoPatch+ retrieves relevant documents via RAG.

LLM answers the question.

Detector checks if answer is hallucinated.

If hallucinated, the pipeline rewrites using facts.

ğŸ§ª Citation
If you use AutoPatch+ in your research, please cite:

bibtex
Copy
Edit
@inprogress{autopatch2025,
  title={AutoPatch+: A Hallucination-Aware Verification and Correction Framework for Large Language Models},
  author={Sree Bhargavi Balija},
  year={2025},
  note={Under submission}
}
ğŸ¤ Contributing
We welcome contributions! Please open issues or submit PRs for improvements. See CONTRIBUTING.md for details.
